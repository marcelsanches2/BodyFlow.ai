"""
Servi√ßo centralizado para comunica√ß√£o com LLMs usando LiteLLM
Suporte a m√∫ltiplos provedores com fallback autom√°tico
"""

import os
import asyncio
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv
import litellm
from litellm import completion, acompletion


class LLMService:
    """Servi√ßo centralizado para comunica√ß√£o com LLMs usando LiteLLM"""
    
    def __init__(self):
        # Carrega vari√°veis de ambiente
        load_dotenv('.env.secrets')
        
        # Configura√ß√£o dos provedores
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        openai_key = os.getenv("OPENAI_API_KEY")
        
        self.providers = {
            "anthropic": {
                "api_key": anthropic_key,
                "model": "claude-3-5-sonnet-20241022",
                "priority": 1
            }
        }
        
        # Adiciona OpenAI apenas se a chave for v√°lida (n√£o placeholder)
        if openai_key and openai_key != "your_openai_api_key_here":
            # Adiciona GPT-4o como segunda op√ß√£o
            self.providers["openai_gpt4o"] = {
                "api_key": openai_key,
                "model": "gpt-4o",
                "priority": 2
            }
            # Adiciona GPT-4o Mini como terceira op√ß√£o
            self.providers["openai_gpt4o_mini"] = {
                "api_key": openai_key,
                "model": "gpt-4o-mini",
                "priority": 3
            }
        
        # Configura√ß√µes do LiteLLM
        self.max_retries = 2
        self.base_delay = 1
        self.timeout = 10
        
        # Configura LiteLLM
        self._setup_litellm()
    
    def _setup_litellm(self):
        """Configura LiteLLM com os provedores dispon√≠veis"""
        # Configura timeout global
        litellm.request_timeout = self.timeout
        
        # Configura retry autom√°tico
        litellm.num_retries = self.max_retries
        
        # Configura fallback autom√°tico
        litellm.drop_params = True
        
        # Configura logging
        litellm.set_verbose = False
    
    async def call_with_fallback(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 2000,
        temperature: float = 0.4,
        fallback_response: Optional[str] = None,
        conversation_context: Optional[str] = None
    ) -> str:
        """
        Chama LLM com fallback autom√°tico entre provedores
        
        Args:
            messages: Lista de mensagens para enviar
            max_tokens: N√∫mero m√°ximo de tokens
            temperature: Temperatura para gera√ß√£o
            fallback_response: Resposta de fallback se todos os provedores falharem
            conversation_context: Contexto da conversa para manter continuidade
            
        Returns:
            Resposta da API ou fallback_response se houver erro
        """
        # Ordena provedores por prioridade
        sorted_providers = sorted(
            self.providers.items(),
            key=lambda x: x[1]["priority"]
        )
        
        # Prepara mensagens com contexto da conversa se fornecido
        enhanced_messages = messages.copy()
        if conversation_context:
            # Adiciona contexto da conversa como primeira mensagem do sistema
            context_message = {
                "role": "system",
                "content": f"CONTEXTO DA CONVERSA: {conversation_context}\n\nMantenha a continuidade desta conversa e responda de forma natural."
            }
            enhanced_messages = [context_message] + enhanced_messages
        
        for provider_name, provider_config in sorted_providers:
            if not provider_config["api_key"]:
                print(f"‚ö†Ô∏è {provider_name} n√£o configurado, pulando...")
                continue
            
            try:
                print(f"üöÄ Tentando {provider_name} ({provider_config['model']})...")
                
                # Log das mensagens para debug
                print(f"üîç LLMService: Mensagens sendo enviadas:")
                for i, msg in enumerate(enhanced_messages):
                    if msg.get("role") == "user" and isinstance(msg.get("content"), list):
                        print(f"   Mensagem {i}: {msg['role']} - {len(msg['content'])} itens")
                        for j, item in enumerate(msg["content"]):
                            if item.get("type") == "text":
                                print(f"     Item {j}: texto ({len(item['text'])} chars)")
                            elif item.get("type") == "image_url":
                                print(f"     Item {j}: imagem (base64)")
                    else:
                        print(f"   Mensagem {i}: {msg['role']} - {len(str(msg.get('content', '')))} chars")
                
                # Configura modelo para LiteLLM (formato universal)
                model_name = provider_config['model']
                
                print(f"üîß LLMService: Modelo configurado: {model_name}")
                
                # Configura API key para o provedor
                if provider_name.startswith("anthropic"):
                    os.environ["ANTHROPIC_API_KEY"] = provider_config["api_key"]
                elif provider_name.startswith("openai_"):
                    os.environ["OPENAI_API_KEY"] = provider_config["api_key"]
                
                # Chama LLM com timeout
                response = await asyncio.wait_for(
                    acompletion(
                        model=model_name,
                        messages=enhanced_messages,
                        max_tokens=max_tokens,
                        temperature=temperature
                    ),
                    timeout=self.timeout
                )
                
                # Extrai resposta
                content = response.choices[0].message.content
                print(f"‚úÖ {provider_name} respondeu com sucesso!")
                return content
                
            except asyncio.TimeoutError:
                print(f"‚è∞ Timeout em {provider_name}")
                continue
                
            except Exception as e:
                error_str = str(e)
                print(f"‚ùå Erro em {provider_name}: {error_str[:200]}...")
                
                # Log espec√≠fico para diferentes tipos de erro
                if "RateLimitError" in error_str:
                    print(f"üö´ Rate limit atingido em {provider_name}")
                elif "Invalid user message" in error_str:
                    print(f"üìù Formato de mensagem inv√°lido em {provider_name}")
                elif "NotFoundError" in error_str:
                    print(f"üîç Modelo n√£o encontrado em {provider_name}")
                elif "AuthenticationError" in error_str:
                    print(f"üîë Erro de autentica√ß√£o em {provider_name}")
                
                # Se √© erro retri√°vel, tenta pr√≥ximo provedor
                if self._is_retryable_error(error_str):
                    print(f"üîÑ Tentando pr√≥ximo provedor...")
                    continue
                else:
                    # Erro n√£o retri√°vel, pula para pr√≥ximo provedor
                    print(f"‚è≠Ô∏è Pulando para pr√≥ximo provedor...")
                    continue
        
        # Todos os provedores falharam
        print("‚ùå Todos os provedores falharam")
        
        # Retorna fallback se fornecido
        if fallback_response:
            return fallback_response
        
        # Fallback padr√£o
        return self._get_default_fallback()
    
    def _is_retryable_error(self, error_str: str) -> bool:
        """Verifica se o erro √© retri√°vel"""
        retryable_patterns = [
            "529",
            "overloaded",
            "rate limit",
            "too many requests",
            "service unavailable",
            "temporarily unavailable",
            "timeout",
            "connection"
        ]
        
        error_lower = error_str.lower()
        return any(pattern in error_lower for pattern in retryable_patterns)
    
    def _get_default_fallback(self) -> str:
        """Resposta de fallback padr√£o quando todos os LLMs falham"""
        return """
ü§ñ **Servi√ßo Temporariamente Indispon√≠vel**

Desculpe, todos os servi√ßos de IA est√£o com alta demanda no momento.

**O que voc√™ pode fazer:**
- Tente novamente em alguns minutos
- Reformule sua pergunta de forma mais simples
- Para orienta√ß√µes b√°sicas, consulte nosso FAQ

**Orienta√ß√µes Gerais de Sa√∫de:**
- Mantenha hidrata√ß√£o adequada (2-3L/dia)
- Consuma prote√≠nas em todas as refei√ß√µes
- Inclua fibras e micronutrientes
- Evite alimentos ultraprocessados
- Mantenha consist√™ncia nos treinos

*Estou trabalhando para melhorar a disponibilidade do servi√ßo.*
"""
    
    def get_contextual_fallback(self, content: str, user_name: str, profile: Dict[str, Any]) -> str:
        """Retorna fallback contextual baseado no conte√∫do da mensagem"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ["treino", "exerc√≠cio", "muscula√ß√£o", "academia"]):
            return self._get_training_fallback_sync(user_name, profile)
        elif any(word in content_lower for word in ["comida", "alimenta√ß√£o", "dieta", "nutri√ß√£o"]):
            return self._get_nutrition_fallback_sync(user_name, profile)
        else:
            return self._get_general_fallback_sync(user_name, profile)
    
    def _get_training_fallback_sync(self, user_name: str, profile: Dict[str, Any]) -> str:
        """Fallback s√≠ncrono para consultas de treino"""
        age = profile.get("age", "N/A")
        goal = profile.get("goal", "N/A")
        training_level = profile.get("training_level", "N/A")
        
        return f"""
üí™ **Orienta√ß√£o de Treino**

{user_name}, vejo que voc√™ est√° falando sobre treino! 

Baseado no seu perfil:
- Objetivo: {goal}
- N√≠vel: {training_level}
- Idade: {age} anos

**Orienta√ß√µes Gerais:**
- Mantenha consist√™ncia nos treinos
- Foque na progress√£o gradual
- Descanse adequadamente entre sess√µes
- Hidrate-se bem durante o treino

*Todos os servi√ßos de IA est√£o com alta demanda no momento. Para orienta√ß√µes mais detalhadas, tente novamente em alguns minutos.*
"""
    
    def _get_nutrition_fallback_sync(self, user_name: str, profile: Dict[str, Any]) -> str:
        """Fallback s√≠ncrono para consultas de nutri√ß√£o"""
        age = profile.get("age", "N/A")
        weight = profile.get("current_weight_kg", "N/A")
        height = profile.get("height_cm", "N/A")
        goal = profile.get("goal", "N/A")
        
        return f"""
ü•ó **Orienta√ß√£o Nutricional**

{user_name}, vejo que voc√™ est√° falando sobre alimenta√ß√£o!

Baseado no seu perfil:
- Objetivo: {goal}
- Peso: {weight} kg
- Altura: {height} cm

**Orienta√ß√µes Gerais:**
- Consuma prote√≠nas em todas as refei√ß√µes
- Mantenha hidrata√ß√£o adequada (2-3L/dia)
- Inclua fibras e micronutrientes
- Evite alimentos ultraprocessados

*Todos os servi√ßos de IA est√£o com alta demanda no momento. Para orienta√ß√µes mais detalhadas, tente novamente em alguns minutos.*
"""
    
    def _get_general_fallback_sync(self, user_name: str, profile: Dict[str, Any]) -> str:
        """Fallback s√≠ncrono geral para consultas"""
        age = profile.get("age", "N/A")
        weight = profile.get("current_weight_kg", "N/A")
        height = profile.get("height_cm", "N/A")
        goal = profile.get("goal", "N/A")
        
        return f"""
üë©‚Äç‚öïÔ∏è **Consulta Nutricional**

{user_name}, estou aqui para te ajudar!

Baseado no seu perfil:
- Idade: {age} anos
- Peso: {weight} kg
- Altura: {height} cm
- Objetivo: {goal}

**Orienta√ß√µes Gerais:**
- Mantenha hidrata√ß√£o adequada (2-3L/dia)
- Consuma prote√≠nas em todas as refei√ß√µes
- Inclua fibras e micronutrientes
- Evite alimentos ultraprocessados

*Todos os servi√ßos de IA est√£o com alta demanda no momento. Para orienta√ß√µes mais detalhadas, tente novamente em alguns minutos.*
"""
    
    def get_available_providers(self) -> List[str]:
        """Retorna lista de provedores dispon√≠veis"""
        available = []
        for provider_name, provider_config in self.providers.items():
            if provider_config["api_key"]:
                available.append(provider_name)
        return available
    
    def get_provider_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre os provedores configurados"""
        info = {}
        for provider_name, provider_config in self.providers.items():
            info[provider_name] = {
                "configured": bool(provider_config["api_key"]),
                "model": provider_config["model"],
                "priority": provider_config["priority"]
            }
        return info


# Inst√¢ncia global do servi√ßo
llm_service = LLMService()
